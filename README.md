# ğŸ§ª AgroWeb - Suite de Pruebas de IntegraciÃ³n Centralizada

## ğŸ“‹ DescripciÃ³n

Suite completa de pruebas de integraciÃ³n para todos los microservicios de AgroWeb. Proporciona un marco unificado para pruebas end-to-end, casos de Ã©xito y error, y generaciÃ³n automÃ¡tica de reportes profesionales.

## ğŸ¯ Objetivos

- **Pruebas Unificadas**: Centralizar todas las pruebas de integraciÃ³n en un repositorio Ãºnico
- **Pruebas Multi-Servicio**: Soportar pruebas para Productos, Usuarios y otros servicios
- **Casos Completos**: Validar respuestas correctas y manejo de errores
- **SimulaciÃ³n Realista**: Recrear escenarios reales de uso del sistema
- **Reportes Profesionales**: Generar PDFs formales con mÃ©tricas, grÃ¡ficos y resultados

## ğŸ—ï¸ Estructura del Proyecto

```
IntegrationTests/
â”œâ”€â”€ conftest.py                         # Fixtures y configuraciÃ³n global
â”œâ”€â”€ pytest.ini                          # ConfiguraciÃ³n global de pytest
â”œâ”€â”€ requirements.txt                    # Dependencias consolidadas
â”œâ”€â”€ setup.py                            # ConfiguraciÃ³n del paquete
â”œâ”€â”€ README.md                           # DocumentaciÃ³n principal
â”œâ”€â”€ CHANGELOG.md                        # Historial de cambios
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ .env.example                    # Plantilla de variables de entorno
â”‚   â”œâ”€â”€ productos/                      # ConfiguraciÃ³n especÃ­fica del servicio de productos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ test_config.py
â”‚   â”œâ”€â”€ usuarios/                       # ConfiguraciÃ³n especÃ­fica del servicio de usuarios (futuro)
â”‚   â””â”€â”€ university_assets/              # Logos y assets de la universidad
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ productos/                      # Pruebas del servicio de gestiÃ³n de productos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ conftest.py                 # Fixtures especÃ­ficas de productos
â”‚   â”‚   â”œâ”€â”€ test_api_integration.py     # Pruebas de integraciÃ³n de API
â”‚   â”‚   â”œâ”€â”€ test_product_lifecycle.py   # Pruebas de ciclo de vida completo
â”‚   â”‚   â”œâ”€â”€ test_error_scenarios.py     # Pruebas de manejo de errores
â”‚   â”‚   â””â”€â”€ test_performance.py         # Pruebas de rendimiento y carga
â”‚   â”œâ”€â”€ usuarios/                       # Pruebas del servicio de usuarios (futuro)
â”‚   â””â”€â”€ integration/                    # Pruebas cross-service (futuro)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ productos/                      # Utilidades especÃ­ficas de productos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api_client.py               # Cliente HTTP para API de productos
â”‚   â”‚   â”œâ”€â”€ test_data.py                # Generadores de datos de prueba
â”‚   â”‚   â””â”€â”€ validators.py               # Validadores de respuesta
â”‚   â”œâ”€â”€ usuarios/                       # Utilidades especÃ­ficas de usuarios (futuro)
â”‚   â””â”€â”€ shared/                         # Utilidades compartidas entre servicios
â”‚
â”œâ”€â”€ reporting/                          # Sistema de reportes
â”‚   â”œâ”€â”€ pdf_generator.py                # Generador de reportes PDF
â”‚   â”œâ”€â”€ metrics_collector.py            # Recolector de mÃ©tricas
â”‚   â””â”€â”€ templates/                      # Plantillas de reportes
â”‚
â””â”€â”€ reports/                           # Reportes generados
    â””â”€â”€ [timestamp]/                   # Reportes organizados por fecha
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Prerrequisitos
- Python 3.8 o superior
- Servicios AgroWeb ejecutÃ¡ndose (Productos, Usuarios)

### 2. Instalar Dependencias
```bash
# Navegar al directorio de pruebas
cd "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests"

# Instalar dependencias (mÃ©todo recomendado)
pip install -r requirements.txt

# O usando setup.py
python setup.py install

# Desde cualquier ubicaciÃ³n:
pip install -r "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests\requirements.txt"
```

### 3. Configurar Variables de Entorno
```bash
# Copiar plantilla de configuraciÃ³n
copy config\.env.example .env

# Editar configuraciÃ³n segÃºn necesidades
# Variables principales:
# - API_BASE_URL_PRODUCTOS=http://localhost:5000
# - API_BASE_URL_USUARIOS=http://localhost:5001
# - API_TIMEOUT=30
```

### 4. Verificar Servicios
```bash
# Verificar servicio de productos
curl http://localhost:5000/health

# Verificar servicio de usuarios
curl http://localhost:5001/health
```

## ğŸ§ª EjecuciÃ³n de Pruebas

### Ruta Completa del Proyecto
```bash
# Navegar al directorio de pruebas de integraciÃ³n
cd "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests"

# Verificar ubicaciÃ³n actual
pwd
```

### Ejecutar Suite Completa
```bash
# Desde el directorio IntegrationTests:
# Todas las pruebas con reporte HTML
python -m pytest --html=reports/integration_report.html --self-contained-html

# Con reporte JSON adicional
python -m pytest --html=reports/integration_report.html --json-report --json-report-file=reports/test_results.json

# Desde cualquier ubicaciÃ³n (ruta absoluta):
cd "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests" && python -m pytest --html=reports/integration_report.html --self-contained-html
```

### Ejecutar Pruebas por Servicio
```bash
# Desde IntegrationTests/
# Solo pruebas del servicio de productos
python -m pytest tests/productos/ -v

# Solo pruebas del servicio de usuarios
python -m pytest tests/usuarios/ -v

# Con marcadores especÃ­ficos
python -m pytest -m "productos and api" -v
python -m pytest -m "error_handling" -v

# Desde cualquier ubicaciÃ³n (rutas absolutas):
cd "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests"
python -m pytest "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests\tests\productos" -v
python -m pytest "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests\tests\usuarios" -v
```

### Ejecutar Pruebas EspecÃ­ficas
```bash
# Desde IntegrationTests/
# Pruebas de integraciÃ³n de API
python -m pytest tests/productos/test_api_integration.py -v

# Pruebas de manejo de errores
python -m pytest tests/productos/test_error_scenarios.py -v

# Filtrar por nombre de prueba
python -m pytest -k "test_create_product" -v

# Pruebas de rendimiento (mÃ¡s lentas)
python -m pytest tests/productos/test_performance.py -v -s

# Desde cualquier ubicaciÃ³n con rutas absolutas:
python -m pytest "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests\tests\productos\test_api_integration.py" -v
python -m pytest "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests\tests\productos\test_error_scenarios.py" -v

# Ejecutar desde directorio del proyecto completo:
cd "d:\UN\2025-1\Ingesoft 2\Proyecto"
python -m pytest IntegrationTests/tests/productos/test_api_integration.py -v
```

### Generar Reportes PDF
```bash
# Desde IntegrationTests/ despuÃ©s de ejecutar las pruebas
python reporting/pdf_generator.py

# Con ruta absoluta desde cualquier ubicaciÃ³n
cd "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests"
python reporting/pdf_generator.py

# O directamente:
python "d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests\reporting\pdf_generator.py"

# El reporte se guardarÃ¡ en: d:\UN\2025-1\Ingesoft 2\Proyecto\IntegrationTests\reports\[timestamp]\
```

## ğŸ“Š Tipos de Pruebas Implementadas

### 1. **Pruebas de API REST** (`test_api_integration.py`)
- âœ… Health check endpoints
- âœ… Operaciones CRUD completas
- âœ… ValidaciÃ³n de respuestas HTTP
- âœ… Consistencia de datos
- âœ… MÃ©tricas de observabilidad
- âœ… Pruebas concurrentes

### 2. **Pruebas de Ciclo de Vida** (`test_product_lifecycle.py`)
- âœ… Flujos end-to-end completos
- âœ… MÃºltiples categorÃ­as de productos
- âœ… Persistencia entre operaciones
- âœ… SimulaciÃ³n de sesiones reales
- âœ… ValidaciÃ³n de integridad de datos

### 3. **Pruebas de Manejo de Errores** (`test_error_scenarios.py`)
- âŒ Datos invÃ¡lidos (400 Bad Request)
- âŒ Content-Type incorrecto (415)
- âŒ Recursos inexistentes (404)
- âŒ Endpoints no disponibles (404)
- âŒ Valores negativos y campos faltantes
- âŒ Estabilidad despuÃ©s de errores

### 4. **Pruebas de Rendimiento** (`test_performance.py`)
- âš¡ Umbrales de tiempo de respuesta
- âš¡ Operaciones concurrentes
- âš¡ Pruebas de carga
- âš¡ AnÃ¡lisis de throughput
- âš¡ Percentiles de rendimiento

## ğŸ“„ Reportes y MÃ©tricas

### Reportes HTML
- Resultados detallados por prueba
- GrÃ¡ficos de tiempo de ejecuciÃ³n
- Logs de fallos y errores
- EstadÃ­sticas de cobertura

### Reportes PDF Profesionales
- Portada con informaciÃ³n universitaria
- Resumen ejecutivo con mÃ©tricas clave
- AnÃ¡lisis detallado por servicio
- GrÃ¡ficos de rendimiento
- Recomendaciones basadas en resultados

### MÃ©tricas JSON
- Datos estructurados para anÃ¡lisis
- IntegraciÃ³n con sistemas de monitoreo
- Tendencias histÃ³ricas
- MÃ©tricas de performance

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Marcadores de Pruebas
```bash
# Marcadores disponibles:
pytest --markers

# Ejemplos de uso:
python -m pytest -m "smoke"              # Pruebas bÃ¡sicas
python -m pytest -m "integration"        # Pruebas de integraciÃ³n
python -m pytest -m "performance"        # Pruebas de rendimiento
python -m pytest -m "productos"          # EspecÃ­ficas de productos
python -m pytest -m "not slow"           # Excluir pruebas lentas
```

### Variables de Entorno
```bash
# ConfiguraciÃ³n de API
export API_BASE_URL_PRODUCTOS="http://localhost:5000"
export API_BASE_URL_USUARIOS="http://localhost:5001"
export API_TIMEOUT="30"

# ConfiguraciÃ³n de pruebas
export MAX_RETRY_ATTEMPTS="3"
export TEST_ENV="integration"
export PYTEST_RUNNING="true"
```

### ConfiguraciÃ³n de Timeouts
```bash
# Timeout por prueba individual
python -m pytest --timeout=300

# Solo para pruebas especÃ­ficas
python -m pytest tests/productos/test_performance.py --timeout=600
```

## ğŸ”„ Agregar Nuevos Servicios

### 1. Crear Estructura para Nuevo Servicio (ej: usuarios)
```
tests/usuarios/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                # Fixtures especÃ­ficas
â”œâ”€â”€ test_user_api.py          # Pruebas de API
â”œâ”€â”€ test_auth.py              # Pruebas de autenticaciÃ³n
â””â”€â”€ test_user_lifecycle.py    # Pruebas end-to-end

utils/usuarios/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ api_client.py             # Cliente HTTP
â”œâ”€â”€ auth_client.py            # Cliente de autenticaciÃ³n
â”œâ”€â”€ test_data.py              # Datos de prueba
â””â”€â”€ validators.py             # Validadores

config/usuarios/
â”œâ”€â”€ __init__.py
â””â”€â”€ test_config.py            # ConfiguraciÃ³n especÃ­fica
```

### 2. Actualizar ConfiguraciÃ³n Global
```ini
# En pytest.ini, agregar marcador:
markers =
    usuarios: Pruebas especÃ­ficas del servicio de usuarios
```

### 3. Implementar PatrÃ³n Consistente
- Seguir la estructura de `productos/`
- Usar los mismos patrones de naming
- Implementar fixtures similares
- Mantener estÃ¡ndares de calidad

## ğŸš€ IntegraciÃ³n CI/CD

### Pipeline BÃ¡sico
```yaml
# Ejemplo para GitHub Actions
steps:
  - name: Install dependencies
    run: pip install -r IntegrationTests/requirements.txt
  
  - name: Run integration tests
    run: |
      cd IntegrationTests
      python -m pytest --html=reports/ci_report.html --json-report
  
  - name: Generate PDF report
    run: python IntegrationTests/reporting/pdf_generator.py
```

### EjecutiÃ³n en Paralelo
```bash
# Pruebas en paralelo con pytest-xdist
python -m pytest -n auto

# Por servicio especÃ­fico
python -m pytest tests/productos/ -n 2
python -m pytest tests/usuarios/ -n 2
```

## ğŸ† CaracterÃ­sticas Avanzadas

### 1. **Observabilidad Completa**
- IntegraciÃ³n con mÃ©tricas Prometheus
- ValidaciÃ³n de health checks
- Monitoreo de performance en tiempo real

### 2. **Datos de Prueba Realistas**
- Generadores de productos colombianos
- CategorÃ­as especÃ­ficas (Papa Criolla, Tomate Chonto)
- Datos consistentes con el dominio de negocio

### 3. **ValidaciÃ³n Exhaustiva**
- Validadores de estructura JSON
- Validadores de lÃ³gica de negocio
- Validadores de performance
- Validadores de consistencia cross-endpoint

### 4. **Reportes Universitarios**
- Formato acadÃ©mico profesional
- Logos y branding institucional
- AnÃ¡lisis estadÃ­stico detallado
- Cumplimiento de estÃ¡ndares acadÃ©micos

## ğŸ¤ ContribuciÃ³n

### EstÃ¡ndares de CÃ³digo
- Seguir PEP 8 para Python
- Documentar todas las funciones
- Incluir docstrings descriptivos
- Mantener cobertura de pruebas

### Agregar Nuevas Pruebas
1. Identificar el servicio objetivo
2. Crear la estructura necesaria
3. Implementar pruebas siguiendo patrones existentes
4. Actualizar documentaciÃ³n
5. Validar con suite completa

## ğŸ“ Soporte

Para dudas sobre:
- **Estructura**: Consultar este README
- **ConfiguraciÃ³n**: Revisar archivos en `config/`
- **ImplementaciÃ³n**: Analizar ejemplos en `tests/productos/`
- **Reportes**: DocumentaciÃ³n en `reporting/`

La suite de pruebas estÃ¡ diseÃ±ada para ser auto-explicativa y seguir patrones consistentes que faciliten su extensiÃ³n y mantenimiento.

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Instalar Dependencias
```bash
# Crear entorno virtual
python -m venv integration_env
integration_env\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Configurar Variables de Entorno
```bash
# Copiar archivo de configuraciÃ³n
copy config\.env.example .env

# Editar configuraciÃ³n segÃºn necesidades
```

### 3. Verificar Servicio Principal
```bash
# Asegurar que el servicio de productos estÃ© ejecutÃ¡ndose
# En directorio principal:
python app.py

# Verificar health check
curl http://localhost:5000/health
```

## ğŸ§ª EjecuciÃ³n de Pruebas

### Ejecutar Suite Completa
```bash
# Ejecutar todas las pruebas con reporte
pytest --html=reports/integration_report.html --json-report --json-report-file=reports/test_results.json

# Generar reporte PDF
python reporting/pdf_generator.py
```

### Ejecutar Pruebas EspecÃ­ficas
```bash
# Solo pruebas de API
pytest tests/test_api_integration.py -v

# Solo pruebas de casos de error
pytest tests/test_error_scenarios.py -v

# Pruebas con filtros
pytest -k "test_create_product" -v
```

### Pruebas de Carga
```bash
# Ejecutar pruebas de rendimiento
locust -f tests/test_performance.py --host=http://localhost:5000
```

## ğŸ“Š Tipos de Pruebas Implementadas

### 1. Pruebas de IntegraciÃ³n de API
- âœ… Crear productos (POST /products)
- âœ… Listar productos (GET /products)
- âœ… Obtener producto por ID (GET /products/{id})
- âœ… Filtrar por categorÃ­a (GET /products/category/{category})
- âœ… Health check (GET /health)
- âœ… MÃ©tricas (GET /metrics)

### 2. Pruebas de Ciclo de Vida Completo
- âœ… Flujo completo: Crear â†’ Listar â†’ Consultar â†’ Validar
- âœ… IntegraciÃ³n con base de datos Cassandra
- âœ… Persistencia de datos entre operaciones

### 3. Pruebas de Casos de Error
- âŒ Datos invÃ¡lidos (400 Bad Request)
- âŒ Productos no encontrados (404 Not Found)
- âŒ Content-Type incorrecto (415 Unsupported Media Type)
- âŒ Errores de servidor (500 Internal Server Error)

### 4. Pruebas de Rendimiento
- ğŸ“ˆ Latencia de respuesta
- ğŸ“ˆ Throughput (peticiones por segundo)
- ğŸ“ˆ Manejo de carga concurrente
- ğŸ“ˆ MÃ©tricas de Prometheus

## ğŸ“„ Reportes Generados

### Reporte PDF Profesional
- **Header universitario** con logo y datos institucionales
- **Resumen ejecutivo** con mÃ©tricas clave
- **Resultados detallados** por categorÃ­a de prueba
- **GrÃ¡ficos de rendimiento** (latencia, throughput, errores)
- **AnÃ¡lisis de cobertura** de endpoints
- **Recomendaciones** basadas en resultados

### MÃ©tricas Incluidas
- âœ… **Tasa de Ã©xito** por endpoint
- â±ï¸ **Latencia promedio** y percentiles
- ğŸ“Š **DistribuciÃ³n de cÃ³digos de estado HTTP**
- ğŸ“ˆ **GrÃ¡ficos de rendimiento temporal**
- ğŸ¯ **Cobertura de casos de prueba**

## ğŸ›ï¸ InformaciÃ³n Institucional

**Universidad Nacional de Colombia**  
**Facultad de IngenierÃ­a**  
**IngenierÃ­a de Software II**  
**Proyecto: AgroWeb - Plataforma de Productos AgrÃ­colas**

**Equipo de Desarrollo:** Capibaras Team  
**Fecha de GeneraciÃ³n:** [Auto-generada en cada ejecuciÃ³n]

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno
```bash
# API Configuration
API_BASE_URL=http://localhost:5000
API_TIMEOUT=30

# Database
DB_WAIT_TIME=60

# Reporting
REPORT_OUTPUT_DIR=reports
INCLUDE_PERFORMANCE_GRAPHS=true
UNIVERSITY_LOGO_PATH=config/university_assets/logo_unal.png

# Test Configuration
MAX_RETRY_ATTEMPTS=3
CONCURRENT_USERS=10
TEST_DURATION_SECONDS=60
```

## ğŸ“š Casos de Uso Probados

1. **Usuario consulta catÃ¡logo de productos**
   - Lista todos los productos disponibles
   - Filtra por categorÃ­a especÃ­fica
   - Verifica formato de respuesta y datos

2. **Usuario busca producto especÃ­fico**
   - Busca producto por ID vÃ¡lido
   - Maneja producto inexistente
   - Valida estructura de respuesta

3. **Administrador agrega nuevo producto**
   - Crea producto con datos vÃ¡lidos
   - Valida creaciÃ³n exitosa
   - Verifica persistencia en base de datos

4. **Sistema maneja errores gracefully**
   - Datos malformados en requests
   - Endpoints inexistentes
   - Sobrecarga del sistema

## ğŸ¯ Criterios de AceptaciÃ³n

- âœ… **Tasa de Ã©xito â‰¥ 95%** para casos de Ã©xito
- âœ… **Latencia promedio â‰¤ 200ms** para operaciones CRUD
- âœ… **Cobertura de errores 100%** para cÃ³digos HTTP esperados
- âœ… **Disponibilidad del sistema â‰¥ 99%** durante pruebas
- âœ… **Reporte PDF generado** sin errores en cada ejecuciÃ³n

---

*Generado automÃ¡ticamente por el Suite de Pruebas de IntegraciÃ³n AgroWeb*
